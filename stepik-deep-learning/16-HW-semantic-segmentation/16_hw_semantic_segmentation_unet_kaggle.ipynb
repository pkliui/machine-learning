{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "16-hw-semantic-segmentation-unet-kaggle.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkliui/machine-learning/blob/master/stepik-deep-learning/16-HW-semantic-segmentation/16_hw_semantic_segmentation_unet_kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uAf3AY9Yq8_"
      },
      "source": [
        "# Semantic segmentation of moles' images by UNet\n",
        "\n",
        "* This is a pytorch implementation of the UNet network for semantic segmentation of moles' images\n",
        "* Dataset: PD2 ADDI database https://www.fc.up.pt/addi/ph2%20database.html \n",
        "* Homework of Unit 16, Deep learning course (part 1, spring 2021) offered by Moscow Institute of Physics and Technology https://stepik.org/course/91157/info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYJr43d4Yq9C"
      },
      "source": [
        "## Fix seed for reproducibility\n",
        "* #torch.use_deterministic_algorithms(True) yields error in kaggle, no error in colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-07-14T07:54:55.200438Z",
          "iopub.execute_input": "2021-07-14T07:54:55.200789Z",
          "iopub.status.idle": "2021-07-14T07:54:56.39131Z",
          "shell.execute_reply.started": "2021-07-14T07:54:55.200757Z",
          "shell.execute_reply": "2021-07-14T07:54:56.390493Z"
        },
        "trusted": true,
        "id": "R57HnLa1Yq9D"
      },
      "source": [
        "import numpy as np\n",
        "import torch, os\n",
        "import random\n",
        "# fix seed for reproducible results\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    #torch.use_deterministic_algorithms(True)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkP9NncXYq9E"
      },
      "source": [
        "## Declare parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:16:23.086265Z",
          "iopub.execute_input": "2021-07-14T09:16:23.086718Z",
          "iopub.status.idle": "2021-07-14T09:16:23.094596Z",
          "shell.execute_reply.started": "2021-07-14T09:16:23.086684Z",
          "shell.execute_reply": "2021-07-14T09:16:23.093306Z"
        },
        "trusted": true,
        "id": "663EYJFEYq9E"
      },
      "source": [
        "SIZE_X = (572, 572) # size of input images\n",
        "SIZE_Y = (388, 388) # size of input segmented images\n",
        "\n",
        "BATCH_SIZE = 8 # batch size\n",
        "\n",
        "\n",
        "TRAIN_SHARE = 100 # size of train set\n",
        "VAL_SHARE = 50# size of val set\n",
        "TEST_SHARE = 50# size of test  set\n",
        "\n",
        "MAX_EPOCHS = 150 # number of epochs \n",
        "LEARNING_RATE = 1e-3 # learning rate\n",
        "\n",
        "SCHEDULER_STEP = 50 # scheduler step\n",
        "SCHEDULER_GAMMA = 0.1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taNLqFbnYq9F"
      },
      "source": [
        "## Read  and resize images\n",
        "> Read images and lesions (segmented images) from the root"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:16:24.157254Z",
          "iopub.execute_input": "2021-07-14T09:16:24.157748Z",
          "iopub.status.idle": "2021-07-14T09:16:25.5879Z",
          "shell.execute_reply.started": "2021-07-14T09:16:24.157714Z",
          "shell.execute_reply": "2021-07-14T09:16:25.586794Z"
        },
        "trusted": true,
        "id": "1DpfM3gBYq9F"
      },
      "source": [
        "images = []\n",
        "lesions = []\n",
        "from skimage.io import imread\n",
        "import os\n",
        "root = '/kaggle/input/ph2databaseaddi/PH2Dataset'\n",
        "\n",
        "for root, dirs, files in os.walk(os.path.join(root, 'PH2 Dataset images')):\n",
        "    if root.endswith('_Dermoscopic_Image'):\n",
        "        images.append(imread(os.path.join(root, files[0])))\n",
        "    if root.endswith('_lesion'):\n",
        "        lesions.append(imread(os.path.join(root, files[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBc8Zl09Yq9G"
      },
      "source": [
        "> Resize images to have the same size as the net expects\n",
        "\n",
        "> Use **nearest neighboour interpolation**. This is important because using any other interpolation \"may result in tampering with the ground truth labels\" [ https://ai.stackexchange.com/questions/6274/how-can-i-deal-with-images-of-variable-dimensions-when-doing-image-segmentation ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:24:17.871977Z",
          "iopub.execute_input": "2021-07-14T09:24:17.872443Z",
          "iopub.status.idle": "2021-07-14T09:24:19.034457Z",
          "shell.execute_reply.started": "2021-07-14T09:24:17.872354Z",
          "shell.execute_reply": "2021-07-14T09:24:19.032962Z"
        },
        "trusted": true,
        "id": "opZVbRlcYq9G"
      },
      "source": [
        "# resize images as required by UNet architecture, resize() automatically normalizes to (0,1)\n",
        "# X = image to be segemnted\n",
        "# Y = segmented image\n",
        "\n",
        "from skimage.transform import resize\n",
        "size_X = SIZE_X\n",
        "size_Y = SIZE_Y\n",
        "\n",
        "import cv2 \n",
        "X = [cv2.resize(x, size_X, interpolation=cv2.INTER_NEAREST) for x in images] \n",
        "Y = [cv2.resize(y, size_Y, interpolation=cv2.INTER_NEAREST)>0.5 for y in lesions] \n",
        "X = X / np.max(X)\n",
        "Y = Y / np.max(Y)\n",
        "\n",
        "\n",
        "# convert to float32\n",
        "import numpy as np\n",
        "X = np.array(X, np.float32)\n",
        "Y = np.array(Y, np.float32)\n",
        "\n",
        "\n",
        "print(f'Loaded {len(X)} images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-12T14:30:37.253752Z",
          "iopub.execute_input": "2021-07-12T14:30:37.254344Z",
          "iopub.status.idle": "2021-07-12T14:30:37.627933Z",
          "shell.execute_reply.started": "2021-07-12T14:30:37.254305Z",
          "shell.execute_reply": "2021-07-12T14:30:37.627012Z"
        },
        "id": "GSS4u3jyYq9H"
      },
      "source": [
        "> draw some images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:20:53.326947Z",
          "iopub.execute_input": "2021-07-14T09:20:53.327388Z",
          "iopub.status.idle": "2021-07-14T09:20:54.364389Z",
          "shell.execute_reply.started": "2021-07-14T09:20:53.327335Z",
          "shell.execute_reply": "2021-07-14T09:20:54.363069Z"
        },
        "trusted": true,
        "id": "S6HIXVW5Yq9H"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "for i in range(6):\n",
        "    plt.subplot(2, 6, i+1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(X[i])\n",
        "\n",
        "    plt.subplot(2, 6, i+7)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(Y[i])\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiB64kE5Yq9I"
      },
      "source": [
        "## Split into train-val-test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:20:59.086564Z",
          "iopub.execute_input": "2021-07-14T09:20:59.086946Z",
          "iopub.status.idle": "2021-07-14T09:20:59.09343Z",
          "shell.execute_reply.started": "2021-07-14T09:20:59.086913Z",
          "shell.execute_reply": "2021-07-14T09:20:59.091999Z"
        },
        "trusted": true,
        "id": "Eoa962KdYq9I"
      },
      "source": [
        "# generate len(X) random indices\n",
        "# from len(X) as it were np.arange(len(X))\n",
        "# False is to generate without replacement (no repetitions)\n",
        "ix = np.random.choice(len(X), len(X), False)\n",
        "#\n",
        "# split generated indices to train-val-test sets as following: 100 test-50 val-50 test\n",
        "# [100, 150] entries indicate where along axis the ix array is split. \n",
        "tr, val, ts = np.split(ix, [TRAIN_SHARE, TRAIN_SHARE+VAL_SHARE])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:21:00.286316Z",
          "iopub.execute_input": "2021-07-14T09:21:00.286756Z",
          "iopub.status.idle": "2021-07-14T09:21:00.294246Z",
          "shell.execute_reply.started": "2021-07-14T09:21:00.286723Z",
          "shell.execute_reply": "2021-07-14T09:21:00.292965Z"
        },
        "trusted": true,
        "id": "ZO9Aycc0Yq9J"
      },
      "source": [
        "assert (len(tr), len(val), len(ts))==(TRAIN_SHARE, VAL_SHARE, TEST_SHARE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:21:00.906224Z",
          "iopub.execute_input": "2021-07-14T09:21:00.906644Z",
          "iopub.status.idle": "2021-07-14T09:21:02.679169Z",
          "shell.execute_reply.started": "2021-07-14T09:21:00.906598Z",
          "shell.execute_reply": "2021-07-14T09:21:02.677078Z"
        },
        "trusted": true,
        "id": "9rLTQJt1Yq9J"
      },
      "source": [
        "# load data using dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# set the batch size\n",
        "batch_size = BATCH_SIZE\n",
        "\n",
        "# set the dataloaders\n",
        "# set drop_last to skip the batches with the # elements < batch size\n",
        "data_tr = DataLoader(list(zip(np.rollaxis(X[tr], 3, 1), Y[tr, np.newaxis])), \n",
        "                     batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "data_val = DataLoader(list(zip(np.rollaxis(X[val], 3, 1), Y[val, np.newaxis])),\n",
        "                      batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "data_ts = DataLoader(list(zip(np.rollaxis(X[ts], 3, 1), Y[ts, np.newaxis])),\n",
        "                     batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:21:02.681553Z",
          "iopub.execute_input": "2021-07-14T09:21:02.682022Z",
          "iopub.status.idle": "2021-07-14T09:21:02.963849Z",
          "shell.execute_reply.started": "2021-07-14T09:21:02.681977Z",
          "shell.execute_reply": "2021-07-14T09:21:02.96269Z"
        },
        "trusted": true,
        "id": "vchjCm8wYq9K"
      },
      "source": [
        "ii=0\n",
        "for X_val,Y_val in data_val:\n",
        "    ii+=1\n",
        "    print(ii)\n",
        "    print(X_val[0].shape)\n",
        "    print(X_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:21:04.446567Z",
          "iopub.execute_input": "2021-07-14T09:21:04.446967Z",
          "iopub.status.idle": "2021-07-14T09:21:04.499126Z",
          "shell.execute_reply.started": "2021-07-14T09:21:04.446935Z",
          "shell.execute_reply": "2021-07-14T09:21:04.497928Z"
        },
        "trusted": true,
        "id": "syCbb0dmYq9K"
      },
      "source": [
        "# use cuda if available\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJuQwE9xYq9L"
      },
      "source": [
        "## Implement UNet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:21:05.266274Z",
          "iopub.execute_input": "2021-07-14T09:21:05.266723Z",
          "iopub.status.idle": "2021-07-14T09:21:14.888204Z",
          "shell.execute_reply.started": "2021-07-14T09:21:05.266692Z",
          "shell.execute_reply": "2021-07-14T09:21:14.886979Z"
        },
        "trusted": true,
        "id": "uDblsJIVYq9L"
      },
      "source": [
        "# install torchvision\n",
        "!pip install torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:21:14.89034Z",
          "iopub.execute_input": "2021-07-14T09:21:14.890862Z",
          "iopub.status.idle": "2021-07-14T09:21:15.068687Z",
          "shell.execute_reply.started": "2021-07-14T09:21:14.890809Z",
          "shell.execute_reply": "2021-07-14T09:21:15.067562Z"
        },
        "trusted": true,
        "id": "_iMQuZH2Yq9L"
      },
      "source": [
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "from time import time\n",
        "\n",
        "from matplotlib import rcParams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:21:15.07191Z",
          "iopub.execute_input": "2021-07-14T09:21:15.072429Z",
          "iopub.status.idle": "2021-07-14T09:21:15.317396Z",
          "shell.execute_reply.started": "2021-07-14T09:21:15.072377Z",
          "shell.execute_reply": "2021-07-14T09:21:15.316155Z"
        },
        "trusted": true,
        "id": "50cIwqcvYq9M"
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # encoder (downsampling)\n",
        "        # Each enc_conv/dec_conv block should look like this:\n",
        "        # nn.Sequential(\n",
        "        #     nn.Conv2d(...),\n",
        "        #     ... (2 or 3 conv layers with relu and batchnorm),\n",
        "        # )\n",
        "        ##################\n",
        "        # encoder layer 0 \n",
        "        #################\n",
        "        # 3, 572, 572\n",
        "        self.e0_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3),\n",
        "            # 64, 570, 570\n",
        "            nn.Conv2d(64, 64, kernel_size=3),\n",
        "            # 64, 568, 568\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        # 64, 568, 568\n",
        "        self.e0_pool =   nn.MaxPool2d(2, stride=2, return_indices=False) \n",
        "        # 64, 284, 284 \n",
        "        #\n",
        "        ##################\n",
        "        # encoder layer 0 - cropping for decoder layer and generating a fake maxpool image tensor to get indices for maxunpool\n",
        "        #################\n",
        "        self.e0_crop = nn.Sequential(\n",
        "            # 64, 568, 568\n",
        "            torchvision.transforms.CenterCrop(392)\n",
        "            # 64, 392,392\n",
        "            )\n",
        "        self.e0_pool_idx =  nn.MaxPool2d(2, stride=2, return_indices=True)\n",
        "        # 64, 196,196\n",
        "        #\n",
        "        #################\n",
        "        # encoder layer1\n",
        "        ################\n",
        "        self.e1_conv = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3),\n",
        "            # 128, 282, 282\n",
        "            nn.Conv2d(128, 128, kernel_size=3),\n",
        "            # 128, 280, 280\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        # 128, 280, 280\n",
        "        self.e1_pool =   nn.MaxPool2d(2, stride=2, return_indices=False)\n",
        "        # 128, 140, 140\n",
        "        #\n",
        "        ###################\n",
        "        # encoder layer 1 - cropping for decoder layer and generating a fake maxpool image tensor to get indices for maxunpool\n",
        "        ##################\n",
        "        self.e1_crop = nn.Sequential(\n",
        "            # 128, 280, 280\n",
        "            torchvision.transforms.CenterCrop(200)\n",
        "            # 128, 200, 200\n",
        "            )\n",
        "        self.e1_pool_idx =  nn.MaxPool2d(2, stride=2, return_indices=True)\n",
        "        # 128, 100, 100\n",
        "        #\n",
        "        ###################\n",
        "        # encoder layer 2\n",
        "        ###################\n",
        "        self.e2_conv = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3),\n",
        "            # 256, 138, 138\n",
        "            nn.Conv2d(256, 256, kernel_size=3),\n",
        "            # 256, 136, 136\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        # 256, 136, 136\n",
        "        self.e2_pool =  nn.MaxPool2d(2, stride=2, return_indices=False)\n",
        "        # 256, 68, 68\n",
        "        #\n",
        "        #################\n",
        "        # encoder layer 2 - cropping for decoder layer and generating a fake maxpool image tensor to get indices for maxunpool\n",
        "        #################\n",
        "        self.e2_crop = nn.Sequential(\n",
        "            # 256, 136, 136\n",
        "            torchvision.transforms.CenterCrop(104)\n",
        "            # 256, 104, 104\n",
        "            )\n",
        "        self.e2_pool_idx =  nn.MaxPool2d(2, stride=2, return_indices=True)\n",
        "        # 256, 52, 52\n",
        "        #\n",
        "        ##################\n",
        "        # encoder layer 3\n",
        "        #################\n",
        "        self.e3_conv = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3),\n",
        "            # 512, 66, 66\n",
        "            nn.Conv2d(512, 512, kernel_size=3),\n",
        "            # 512, 64, 64\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        self.e3_pool =  nn.MaxPool2d(2, stride=2, return_indices=False)\n",
        "        # 512, 32, 32\n",
        "        #\n",
        "        #################\n",
        "        # encoder layer 3 - cropping for decoder layer and generating a fake maxpool image tensor to get indices for maxunpool\n",
        "        #################\n",
        "        self.e3_crop = nn.Sequential(\n",
        "            # 512, 64, 64\n",
        "            torchvision.transforms.CenterCrop(56)\n",
        "            # 512, 56, 56\n",
        "            )\n",
        "        self.e3_pool_idx =  nn.MaxPool2d(2, stride=2, return_indices=True)\n",
        "        # 512, 28, 28\n",
        "        #\n",
        "        ###\n",
        "        # bottleneck\n",
        "        ###\n",
        "        # 512, 32, 32\n",
        "        self.bottleneck_conv = nn.Sequential(\n",
        "            nn.Conv2d(512, 1024, kernel_size=3),\n",
        "            # 1024, 30, 30\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3),\n",
        "            # 1024, 28, 28\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "            # 1024, 28, 28\n",
        "\n",
        "        # decoder (upsampling)\n",
        "        ###################\n",
        "        # decoder layer 3\n",
        "        ###################\n",
        "        # 1024, 28, 28--> 255         \n",
        "        self.d3_upsample = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        #self.d3_upsample =  nn.Upsample(scale_factor=2)\n",
        "        # 1024, 56, 56\n",
        "        self.d3_upconv = nn.Sequential(\n",
        "          # H_out=stride*(H_in−1)−2×padding+kernel_size+output_padding \n",
        "          # 56 = 1*(56-1)-2*1+2+1 \n",
        "          #nn.ConvTranspose2d(1024, 512, kernel_size=2, padding=1, output_padding = 1),\n",
        "          # 56 = 1*(56-1)-2*1+3\n",
        "          nn.ConvTranspose2d(1024, 512, kernel_size=3, padding=1),\n",
        "          # 512, 56, 56\n",
        "          nn.BatchNorm2d(512),\n",
        "          nn.ReLU()\n",
        "          # 512, 56, 56\n",
        "          # 1024, 56, 56 after concatenation w/ corresponding cropped encoder map\n",
        "          )\n",
        "        # 1024, 56, 56 \n",
        "        #\n",
        "        self.d3_conv = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=3),\n",
        "            ## 56- 3 + 1\n",
        "            # 512, 54 ,54\n",
        "            nn.Conv2d(512, 512, kernel_size=3),\n",
        "            # 512, 52, 52\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU()\n",
        "            # 512, 52, 52\n",
        "            )\n",
        "        #\n",
        "        ##################\n",
        "        # decoder layer 2 \n",
        "        ##################\n",
        "        # 512, 52, 52\n",
        "        self.d2_upsample =  nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        #self.d2_upsample =  nn.Upsample(scale_factor=2)\n",
        "        # 512, 104, 104\n",
        "        self.d2_upconv = nn.Sequential(\n",
        "        #nn.ConvTranspose2d(512, 256, kernel_size=2, padding=1, output_padding = 1),\n",
        "        nn.ConvTranspose2d(512, 256, kernel_size=3, padding=1),\n",
        "        # 256, 104, 104\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU()\n",
        "        # 256, 104, 104\n",
        "        # 512, 104, 104 after concatenation w/ corresponding cropped encoder map\n",
        "        )\n",
        "        #\n",
        "        self.d2_conv = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3),\n",
        "            # 256, 102 ,102\n",
        "            nn.Conv2d(256, 256, kernel_size=3),\n",
        "            # 256, 100, 100\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "            # 256, 100, 100\n",
        "            )\n",
        "        ##################\n",
        "        # decoder layer 1\n",
        "        ##################\n",
        "        # 256, 100, 100\n",
        "        self.d1_upsample =   nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        #self.d1_upsample =  nn.Upsample(scale_factor=2)\n",
        "        # 256, 200, 200\n",
        "        self.d1_upconv = nn.Sequential(\n",
        "            #nn.ConvTranspose2d(256, 128, kernel_size=2, padding=1, output_padding = 1),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1),\n",
        "            # 128, 200, 200\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "            # 128, 200, 200\n",
        "            # 256, 200, 200 after concatenation w/ corresponding cropped encoder map\n",
        "            )\n",
        "        #\n",
        "        self.d1_conv = nn.Sequential(\n",
        "            # 256, 200, 200\n",
        "            nn.Conv2d(256, 128, kernel_size=3),\n",
        "            # 128, 198, 198\n",
        "            nn.Conv2d(128, 128, kernel_size=3),\n",
        "            # 128, 196, 196\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "            # 128, 196, 196\n",
        "            )\n",
        "        ###\n",
        "        # decoder layer 0\n",
        "        ###\n",
        "        # 128, 196, 196\n",
        "        self.d0_upsample =   nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
        "        #self.d0_upsample =  nn.Upsample(scale_factor=2)\n",
        "        # 128, 392, 392\n",
        "        self.d0_upconv = nn.Sequential(\n",
        "          #nn.ConvTranspose2d(128, 64, kernel_size=2, padding=1, output_padding = 1),\n",
        "          nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1),\n",
        "          # 64, 392, 392\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.ReLU()\n",
        "          # 64, 392, 392\n",
        "          # 128, 392, 392 after concatenation w/ corresponding cropped encoder map\n",
        "          )\n",
        "        #\n",
        "        self.d0_conv = nn.Sequential(\n",
        "            # 128, 392, 392\n",
        "            nn.Conv2d(128, 64, kernel_size=3),\n",
        "            # 64, 390, 390\n",
        "            nn.Conv2d(64, 64, kernel_size=3),\n",
        "            # 64, 388, 388\n",
        "            nn.Conv2d(64, 1, kernel_size=1),\n",
        "            # 1, 388, 388\n",
        "            nn.BatchNorm2d(1),\n",
        "            # 1, 388, 388\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder\n",
        "        ###################\n",
        "        # encoder layer 0\n",
        "        ##################\n",
        "        #\n",
        "        # convolutions\n",
        "        #--> # 1, 572, 572\n",
        "        e0 = self.e0_conv(x)\n",
        "        # --> # 64, 568, 568 \n",
        "        assert (e0.shape[1],e0.shape[2],e0.shape[3])==(64, 568, 568), \"encoder layer e0 expected shape {}, got{}\".format(\"(64, 568, 568)\",(e0.shape[1],e0.shape[2],e0.shape[3]))\n",
        "        #\n",
        "        # pooling\n",
        "        e0_pool = self.e0_pool(e0)\n",
        "        # --> # 64, 284, 284 \n",
        "        assert (e0_pool.shape[1],e0_pool.shape[2],e0_pool.shape[3])==(64, 284, 284), \"encoder layer e0 after pooling expected shape{}\".format(\"(64, 284, 284)\")\n",
        "        #\n",
        "        # cropping and cropped indices for skip connections\n",
        "        e0_crop = self.e0_crop(e0)\n",
        "        # --> # 64, 392,392\n",
        "        _, idx0_crop = self.e0_pool_idx(e0_crop) # --> pooling indices from 64, 392,392 cropped map \n",
        "        # --> # 64, 196,196\n",
        "        assert (e0_crop.shape[1],e0_crop.shape[2],e0_crop.shape[3])==(64, 392, 392), \"encoder layer e0 after cropping expected shape{}\".format(\"(64, 392, 392)\")\n",
        "        assert (idx0_crop.shape[1],idx0_crop.shape[2],idx0_crop.shape[3])==(64, 196, 196), \"encoder indices idx0 after cropping expected shape {}, got {}\".format(\"(64, 196, 196)\",(idx0_crop.shape[1],idx0_crop.shape[2],idx0_crop.shape[3]))\n",
        "        #\n",
        "        ##################\n",
        "        # encoder layer 1\n",
        "        ##################\n",
        "        #  convolutions\n",
        "        # --> # 64, 284, 284\n",
        "        e1 = self.e1_conv(e0_pool)\n",
        "        # --> # 128, 280, 280\n",
        "        assert (e1.shape[1],e1.shape[2],e1.shape[3])==(128, 280, 280), \"encoder layer e1 expected shape {}\".format(\"(128, 280, 280)\")\n",
        "        #\n",
        "        # pooling\n",
        "        e1_pool = self.e1_pool(e1)\n",
        "        # --> # 128, 140, 140\n",
        "        assert (e1_pool.shape[1],e1_pool.shape[2],e1_pool.shape[3])==(128, 140, 140), \"encoder layer e1 after pooling expected shape{}\".format(\"(128, 140, 140)\")\n",
        "        #\n",
        "        # cropping and cropped indices for skip connections\n",
        "        e1_crop = self.e1_crop(e1)\n",
        "        # --> # 128, 200, 200\n",
        "        _, idx1_crop = self.e1_pool_idx(e1_crop) # --> pooling indices from 128, 200, 200 cropped map\n",
        "        # --> # 128, 100, 100\n",
        "        assert (e1_crop.shape[1],e1_crop.shape[2],e1_crop.shape[3])==(128, 200, 200), \"encoder layer e1 after cropping expected shape{}\".format(\"(128, 200, 200)\")\n",
        "        assert (idx1_crop.shape[1],idx1_crop.shape[2],idx1_crop.shape[3])==(128, 100, 100), \"encoder indices idx1 after cropping expected shape{}\".format(\"(128, 100, 100)\")\n",
        "        #\n",
        "        ##################\n",
        "        # encoder layer 2\n",
        "        ##################\n",
        "        #  convolutions\n",
        "        # --> # 128, 140, 140\n",
        "        e2 = self.e2_conv(e1_pool)\n",
        "        # --> # 256, 136, 136\n",
        "        assert (e2.shape[1],e2.shape[2],e2.shape[3])==(256, 136, 136), \"encoder layer e2 expected shape {}\".format(\"(256, 136, 136)\")\n",
        "        #\n",
        "        # pooling\n",
        "        e2_pool = self.e2_pool(e2) \n",
        "        # --> # 256, 68, 68\n",
        "        assert (e2_pool.shape[1],e2_pool.shape[2],e2_pool.shape[3])==(256, 68, 68), \"encoder layer e2 after pooling expected shape{}\".format(\"(256, 68, 68)\")\n",
        "        #\n",
        "        # cropping and cropped indices for skip connections\n",
        "        e2_crop = self.e2_crop(e2) \n",
        "        # --> # 256, 104, 104\n",
        "        _, idx2_crop = self.e2_pool_idx(e2_crop) # --> pooling indices from 256, 104, 104 cropped map\n",
        "        # --> # 256, 52, 52\n",
        "        assert (e2_crop.shape[1],e2_crop.shape[2],e2_crop.shape[3])==(256, 104, 104), \"encoder layer e2 after cropping expected shape{}\".format(\"(256, 104, 104)\")\n",
        "        assert (idx2_crop.shape[1],idx2_crop.shape[2],idx2_crop.shape[3])==(256, 52, 52), \"encoder indices idx2 after cropping expected shape{}\".format(\"(256, 52, 52)\")\n",
        "        #\n",
        "        ##################\n",
        "        # encoder layer 3\n",
        "        ##################\n",
        "        #  convolutions\n",
        "        # --> # 256, 68, 68\n",
        "        e3 = self.e3_conv(e2_pool)\n",
        "        # --> # 512, 64, 64\n",
        "        assert (e3.shape[1],e3.shape[2],e3.shape[3])==(512, 64, 64), \"encoder layer e3 expected shape {}\".format(\"(512, 64, 64)\")\n",
        "        #\n",
        "        # pooling\n",
        "        e3_pool = self.e3_pool(e3) \n",
        "        # --> # 512, 32, 32\n",
        "        assert (e3_pool.shape[1],e3_pool.shape[2],e3_pool.shape[3])==(512, 32, 32), \"encoder layer e3 after pooling expected shape{}\".format(\"(512, 32, 32)\")\n",
        "        #\n",
        "        # cropping and cropped indices for skip connections\n",
        "        e3_crop = self.e3_crop(e3) \n",
        "        # --> # 512, 56, 56\n",
        "        _, idx3_crop = self.e3_pool_idx(e3_crop) # --> pooling indices from 512, 56, 56 cropped map\n",
        "        # --> # 512, 28, 28\n",
        "        assert (e3_crop.shape[1],e3_crop.shape[2],e3_crop.shape[3])==(512, 56, 56), \"encoder layer e3 after cropping expected shape{}\".format(\"(512, 56, 56)\")\n",
        "        assert (idx3_crop.shape[1],idx3_crop.shape[2],idx3_crop.shape[3])==(512, 28, 28), \"encoder indices idx3 after cropping expected shape{}\".format(\"(512, 28, 28)\")\n",
        "        #\n",
        "        # bottleneck\n",
        "        # --> # 512, 32, 32\n",
        "        b = self.bottleneck_conv(e3_pool) \n",
        "        # --> # 1024, 28, 28\n",
        "        assert (b.shape[1],b.shape[2],b.shape[3])==(1024, 28, 28), \"bottleneck expected shape{}\".format(\"(1024, 28, 28)\")\n",
        "        #\n",
        "        # decoder\n",
        "        ##################\n",
        "        # decoder layer 3 (reverse counting order)\n",
        "        ##################\n",
        "        #\n",
        "        # upconvolution\n",
        "        d3_upconv = self.d3_upconv(b) \n",
        "        # --> # 512, 28, 28\n",
        "        assert (d3_upconv.shape[1],d3_upconv.shape[2],d3_upconv.shape[3])==(512, 28, 28), \"decoder layer d3 after upconvolution expected shape{}\".format(\"(512, 28, 28)\")\n",
        "        #\n",
        "        # upsampling  idx3 - 512, 28, 28\n",
        "        # --> # 512, 28, 28 \n",
        "        d3_upsample = self.d3_upsample(d3_upconv,idx3_crop) \n",
        "        #d3_upsample = self.d3_upsample(b) \n",
        "        # --> # 512, 56, 56     \n",
        "        assert (d3_upsample.shape[1],d3_upsample.shape[2],d3_upsample.shape[3])==(512, 56, 56), \"decoder layer d3 after upsampling expected shape{}\".format(\"(512, 56, 56)\")\n",
        "        #\n",
        "        # concatenation\n",
        "        d3_concat = torch.cat((e3_crop,d3_upsample),dim=1) \n",
        "        # -->  512,56,56 + 512,56,56 = 1024,56,56\n",
        "        assert (d3_concat.shape[1],d3_concat.shape[2],d3_concat.shape[3])==(1024,56,56), \"decoder layer d3 after concatenation expected shape{}\".format(\"(1024,56,56)\")\n",
        "        #\n",
        "        # convolution\n",
        "        d3 = self.d3_conv(d3_concat) \n",
        "        # -->    # 512, 52, 52 \n",
        "        assert (d3.shape[1],d3.shape[2],d3.shape[3])==(512, 52, 52), \"decoder layer d3 final expected shape{}\".format(\"(512, 52, 52)\")\n",
        "        #\n",
        "        ##################\n",
        "        # decoder layer 2\n",
        "        ##################\n",
        "        #\n",
        "        # upconvolution\n",
        "        d2_upconv = self.d2_upconv(d3) \n",
        "        # --> # 256, 52, 52\n",
        "        assert (d2_upconv.shape[1],d2_upconv.shape[2],d2_upconv.shape[3])==(256, 52, 52), \"decoder layer d2 after upconvolution expected shape{}\".format(\"(256, 52, 52)\")\n",
        "        #\n",
        "        # upsampling - idx2 - # 256, 52, 52\n",
        "        d2_upsample = self.d2_upsample(d2_upconv,idx2_crop) \n",
        "        #d2_upsample = self.d2_upsample(d3) \n",
        "        # --> 256, 104, 104\n",
        "        assert (d2_upsample.shape[1],d2_upsample.shape[2],d2_upsample.shape[3])==(256, 104, 104), \"decoder layer d2 after upsampling expected shape{}\".format(\"(256, 104, 104)\")\n",
        "        #\n",
        "        # concatenation\n",
        "        d2_concat = torch.cat((e2_crop,d2_upsample),dim=1) \n",
        "        # -->  256, 104, 104 + 256, 104, 104 = 512, 104, 104\n",
        "        assert (d2_concat.shape[1],d2_concat.shape[2],d2_concat.shape[3])==(512, 104, 104), \"decoder layer d2 after concatenation expected shape{}\".format(\"(512, 104, 104)\")\n",
        "        #\n",
        "        # convolution\n",
        "        d2 = self.d2_conv(d2_concat)   \n",
        "        # 256, 100, 100 \n",
        "        assert (d2.shape[1],d2.shape[2],d2.shape[3])==(256, 100, 100 ), \"decoder layer d2 final expected shape{}\".format(\"(256, 100, 100 )\")\n",
        "        #\n",
        "        ##################\n",
        "        # decoder layer 1\n",
        "        ##################\n",
        "        #\n",
        "        # upconvolution\n",
        "        d1_upconv = self.d1_upconv(d2) \n",
        "        # --> # 128, 100, 100\n",
        "        assert (d1_upconv.shape[1],d1_upconv.shape[2],d1_upconv.shape[3])==(128, 100, 100), \"decoder layer d1 after upconvolution expected shape{}\".format(\"(128,100,100)\")\n",
        "        #\n",
        "        # upsampling\n",
        "        d1_upsample = self.d1_upsample(d1_upconv,idx1_crop) \n",
        "        #d1_upsample = self.d1_upsample(d2) \n",
        "        # --> 128, 200, 200\n",
        "        assert (d1_upsample.shape[1],d1_upsample.shape[2],d1_upsample.shape[3])==(128, 200, 200), \"decoder layer d1 after upsampling expected shape{}\".format(\"(128, 200, 200)\")\n",
        "        #\n",
        "        # concatenation\n",
        "        d1_concat = torch.cat((e1_crop,d1_upsample),dim=1) \n",
        "        # -->  128, 200, 200 + 128, 200, 200 = 256, 200, 200\n",
        "        assert (d1_concat.shape[1],d1_concat.shape[2],d1_concat.shape[3])==(256, 200, 200), \"decoder layer d1 after concatenation expected shape{}\".format(\"(256, 200, 200)\")\n",
        "        #\n",
        "        # convolution\n",
        "        d1 = self.d1_conv(d1_concat)   \n",
        "        # -->    # 128, 196, 196 \n",
        "        assert (d1.shape[1],d1.shape[2],d1.shape[3])==(128, 196, 196), \"decoder layer d1 final expected shape{}\".format(\"(128, 196, 196)\")\n",
        "        #\n",
        "        ##################\n",
        "        # decoder layer 0\n",
        "        ##################\n",
        "        #\n",
        "        # upconvolution\n",
        "        d0_upconv = self.d0_upconv(d1) \n",
        "        # --> # 64, 196, 196\n",
        "        assert (d0_upconv.shape[1],d0_upconv.shape[2],d0_upconv.shape[3])==(64, 196, 196), \"decoder layer d0 after upconvolution expected shape{}\".format(\"(64, 196, 196)\")\n",
        "        #\n",
        "        # upsampling\n",
        "        d0_upsample = self.d0_upsample(d0_upconv,idx0_crop) \n",
        "        #d0_upsample = self.d0_upsample(d1) \n",
        "        # --> 64, 392, 392\n",
        "        assert (d0_upsample.shape[1],d0_upsample.shape[2],d0_upsample.shape[3])==(64, 392, 392), \"decoder layer d0 after upsampling expected shape{}\".format(\"(64, 392, 392)\")\n",
        "#\n",
        "        # concatenation\n",
        "        d0_concat = torch.cat((e0_crop,d0_upsample),dim=1) \n",
        "        # -->  64, 392, 392 + 64, 392, 392 = 128, 392, 392\n",
        "        assert (d0_concat.shape[1],d0_concat.shape[2],d0_concat.shape[3])==(128, 392, 392), \"decoder layer d0 after concatenation expected shape{}\".format(\"(128, 392, 392)\")\n",
        "        #\n",
        "        # convolution\n",
        "        d0 = self.d0_conv(d0_concat) \n",
        "        # -->    # 1,388,388\n",
        "        assert (d0.shape[1],d0.shape[2],d0.shape[3])==(1,388,388), \"decoder layer d0 final expected shape{}\".format(\"(1,388,388)\")\n",
        "        \n",
        "        # return d0 output\n",
        "        return d0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:21:15.320291Z",
          "iopub.execute_input": "2021-07-14T09:21:15.320717Z",
          "iopub.status.idle": "2021-07-14T09:21:15.338183Z",
          "shell.execute_reply.started": "2021-07-14T09:21:15.320682Z",
          "shell.execute_reply": "2021-07-14T09:21:15.336475Z"
        },
        "trusted": true,
        "id": "Kn9-hMMOYq9O"
      },
      "source": [
        "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
        "    # You can comment out this line if you are passing tensors of equal shape\n",
        "    # But if you are passing output from UNet or something it will most probably\n",
        "    # be with the BATCH x 1 x H x W shape\n",
        "    outputs = outputs.squeeze(1).byte()  # BATCH x 1 x H x W => BATCH x H x W\n",
        "    labels = labels.squeeze(1).byte()\n",
        "    SMOOTH = 1e-8\n",
        "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
        "    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n",
        "    \n",
        "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
        "    \n",
        "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
        "    \n",
        "    return thresholded  # "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOFXaXqfYq9O"
      },
      "source": [
        "## Define segmentation metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:21:15.340013Z",
          "iopub.execute_input": "2021-07-14T09:21:15.34072Z",
          "iopub.status.idle": "2021-07-14T09:21:15.353058Z",
          "shell.execute_reply.started": "2021-07-14T09:21:15.340557Z",
          "shell.execute_reply": "2021-07-14T09:21:15.351708Z"
        },
        "trusted": true,
        "id": "uTQqhhn_Yq9P"
      },
      "source": [
        "bce_loss = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0TrbS2Yq9P"
      },
      "source": [
        "## Setup training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:21:15.354935Z",
          "iopub.execute_input": "2021-07-14T09:21:15.35548Z",
          "iopub.status.idle": "2021-07-14T09:21:15.383289Z",
          "shell.execute_reply.started": "2021-07-14T09:21:15.355428Z",
          "shell.execute_reply": "2021-07-14T09:21:15.381578Z"
        },
        "trusted": true,
        "id": "laGVB6F1Yq9P"
      },
      "source": [
        "def train(model, opt, loss_fn, epochs, batch_size, data_tr, data_val, metric, lr_sched=None):\n",
        "    \"\"\"\n",
        "    - trains the model\n",
        "    - computes training loss, validation loss and validation score\n",
        "    - plots the progress\n",
        "    ---\n",
        "    input\n",
        "    ---\n",
        "    model: \n",
        "        pytorch  model to train\n",
        "    opt: \n",
        "        optimizer\n",
        "    loss_fn : \n",
        "        loss function\n",
        "    epochs: int\n",
        "        number of epochs\n",
        "    data_tr: dataloader\n",
        "        training data\n",
        "    data_val: dataloader\n",
        "        validation data\n",
        "    metric: \n",
        "        segmentation metric\n",
        "    lr_sched: \n",
        "        scheduler\n",
        "        default: None\n",
        "    ---\n",
        "    return\n",
        "    ---\n",
        "    training loss, validation loss and validation score\n",
        "    \"\"\"\n",
        "    # loss and score declaration\n",
        "    train_loss_values = []\n",
        "    val_loss_values = []\n",
        "    avg_score_values = []\n",
        "    #\n",
        "    # iterate through epochs\n",
        "    for epoch in range(epochs):\n",
        "        #\n",
        "        # epochs counter\n",
        "        ii = 0\n",
        "        print('* Epoch %d/%d' % (epoch+1, epochs))\n",
        "        # \n",
        "        # average loss declaration\n",
        "        avg_loss = 0\n",
        "        val_avg_loss = 0\n",
        "        #\n",
        "        ##############\n",
        "        # train model \n",
        "        ##############\n",
        "        #\n",
        "        model.train() \n",
        "        #\n",
        "        for X_batch, Y_batch in data_tr:\n",
        "            #print(\"batch \", ii, \" out of \", len(data_tr) )\n",
        "            # epochs counter\n",
        "            ii+=1\n",
        "            #print(\"X_batch.shape from data_tr\",X_batch.shape)\n",
        "            #print(\"Y_batch.shape from data_tr\",Y_batch.shape)\n",
        "          #\n",
        "            # data to device\n",
        "            X_batch = X_batch.to(device)\n",
        "            #print(\"X_batch.shape to device\",X_batch.shape)\n",
        "            Y_batch = Y_batch.to(device)\n",
        "            #print(\"Y_batch.shape to device\",Y_batch.shape)\n",
        "            #\n",
        "            # set parameter gradients to zero\n",
        "            opt.zero_grad()\n",
        "            #\n",
        "            # forward propagation\n",
        "            #\n",
        "            # get logits\n",
        "            Y_pred = model(X_batch)\n",
        "            #print(\"Y_pred.shape\",Y_pred.shape)\n",
        "            #\n",
        "            # compute train loss\n",
        "            loss =  loss_fn(Y_pred,Y_batch) # forward-pass - BCEWithLogitsLoss (pred,prob)\n",
        "            #\n",
        "            # backward-pass\n",
        "            #\n",
        "            loss.backward()  \n",
        "            # update weights\n",
        "            opt.step()  \n",
        "            #\n",
        "            # calculate loss to show the user\n",
        "            avg_loss += loss \n",
        "        avg_loss = avg_loss / len(data_tr)\n",
        "        #\n",
        "        print('loss: %f' % avg_loss)\n",
        "        # append train loss\n",
        "        train_loss_values.append(avg_loss.detach().cpu().numpy())\n",
        "\n",
        "        #\n",
        "        # validate model\n",
        "        #\n",
        "        with torch.no_grad():\n",
        "            # \n",
        "            # set dropout and batch normalization layers to evaluation mode before running inference\n",
        "            model.eval()  \n",
        "            score = 0\n",
        "            avg_score = 0\n",
        "          #\n",
        "            for X_val, Y_val in data_val:\n",
        "                # get logits for val set\n",
        "                Y_hat =  model(X_val.to(device)).detach().to('cpu')\n",
        "                #\n",
        "                # only for plotting purposes: apply sigmoid and round to the nearest integer (0,1)\n",
        "                # to obtain binary image\n",
        "                Y_hat_2plot = torch.round(torch.sigmoid(Y_hat))\n",
        "                #Y_hat = torch.round(torch.sigmoid(Y_hat))\n",
        "\n",
        "                #print(\"Y hat shape\", Y_hat.shape)\n",
        "                #\n",
        "                # compute val loss and append it\n",
        "                val_loss =  loss_fn(Y_hat, Y_val)\n",
        "                val_avg_loss += val_loss \n",
        "                #\n",
        "                # compute score for the current batch\n",
        "                #score += metric(Y_hat_2plot.to(device), Y_val.to(device)).mean().item()\n",
        "                # temporarily replace by metric().mean without .item() because this leads to float has no attribute detach error \n",
        "                # see https://github.com/horovod/horovod/issues/852\n",
        "                score += metric(Y_hat_2plot.to(device), Y_val.to(device)).mean()\n",
        "            #\n",
        "            # compute and append average val loss at current epoch\n",
        "            val_avg_loss = val_avg_loss / len(data_val)\n",
        "            val_loss_values.append(val_avg_loss.detach().cpu().numpy())\n",
        "            #\n",
        "            # compute and append average score at current epoch\n",
        "            avg_score = score/len(data_val)\n",
        "            avg_score_values.append(avg_score.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        # plotting\n",
        "        num_images_to_plot = 5 * (batch_size > 5) + batch_size * (batch_size <= 5)\n",
        "        rcParams['figure.figsize'] = (2*num_images_to_plot,2*4)\n",
        "        #\n",
        "        for k in range(num_images_to_plot):\n",
        "            # subplot (height, width, absolute image position)\n",
        "            plt.subplot(4, num_images_to_plot, k+1)\n",
        "            plt.imshow(np.rollaxis(X_val[k].numpy(), 0, 3), cmap='gray')\n",
        "            plt.title('Input image')\n",
        "            plt.axis('off')\n",
        "\n",
        "\n",
        "            plt.subplot(4, num_images_to_plot, k+num_images_to_plot+1)\n",
        "            plt.imshow(Y_hat[k, 0], cmap='gray')\n",
        "            plt.title('Output')\n",
        "            plt.axis('off')\n",
        "\n",
        "\n",
        "            plt.subplot(4, num_images_to_plot, k+num_images_to_plot*2+1)\n",
        "            plt.imshow(Y_hat_2plot[k, 0], cmap='gray')\n",
        "            plt.title('Binary output')\n",
        "            plt.axis('off')\n",
        "\n",
        "\n",
        "            plt.subplot(4, num_images_to_plot, k+num_images_to_plot*3+1)\n",
        "            plt.imshow(Y_val[k, 0], cmap='gray')\n",
        "            plt.title('Ground truth')\n",
        "            plt.axis('off')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "\n",
        "        plt.suptitle('%d / %d - train loss: %f' % (epoch+1, epochs, avg_loss))\n",
        "        plt.suptitle('%d / %d - val. loss: %f' % (epoch+1, epochs, val_avg_loss))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        # CHANGES HERE\n",
        "        # make a scheduler step if required\n",
        "        if lr_sched != None:\n",
        "            lr_sched.step()\n",
        "        # CHANGES END\n",
        "\n",
        "    plt.plot(train_loss_values)\n",
        "    plt.plot(val_loss_values)\n",
        "    plt.plot(avg_score_values)\n",
        "    plt.legend([\"train_loss\", \"val_loss\", \"val_score\"], loc =\"lower right\")\n",
        "    plt.show\n",
        "\n",
        "    return train_loss_values, val_loss_values, avg_score_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:21:15.385162Z",
          "iopub.execute_input": "2021-07-14T09:21:15.385774Z",
          "iopub.status.idle": "2021-07-14T09:21:15.402056Z",
          "shell.execute_reply.started": "2021-07-14T09:21:15.385726Z",
          "shell.execute_reply": "2021-07-14T09:21:15.400926Z"
        },
        "trusted": true,
        "id": "KkeXB8ItYq9Q"
      },
      "source": [
        "def score_model(model, metric, data):\n",
        "    \"\"\"\n",
        "    computes model's score using provided metric and data\n",
        "    ---\n",
        "    return\n",
        "    ---\n",
        "    scores/len(data): float\n",
        "        model's score\n",
        "    \"\"\"\n",
        "    # set dropout and batch normalization layers to evaluation mode before running inference\n",
        "    model.eval()\n",
        "    scores = 0\n",
        "    # iterate thru data\n",
        "    for X_batch, Y_label in data:\n",
        "      # no gradient for validation\n",
        "        with torch.no_grad():\n",
        "          #\n",
        "          # predict\n",
        "            Y_pred = model(X_batch.to(device))\n",
        "          #\n",
        "          # compute sigmoid and round to the nearest integer (0,1) \n",
        "          # to be able to compare with the binary ground truth images\n",
        "            Y_pred = torch.round(torch.sigmoid(Y_pred))\n",
        "            \n",
        "        scores += metric(Y_pred, Y_label.to(device)).mean().item()\n",
        "\n",
        "    return scores/len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:21:15.405674Z",
          "iopub.execute_input": "2021-07-14T09:21:15.406143Z",
          "iopub.status.idle": "2021-07-14T09:21:20.977004Z",
          "shell.execute_reply.started": "2021-07-14T09:21:15.406096Z",
          "shell.execute_reply": "2021-07-14T09:21:20.975876Z"
        },
        "trusted": true,
        "id": "ztgMF4yIYq9R"
      },
      "source": [
        "# send model to device \n",
        "unet_model = UNet().to(device)\n",
        "\n",
        "# define \n",
        "max_epochs = MAX_EPOCHS\n",
        "batch_size = BATCH_SIZE\n",
        "bce_loss = nn.BCEWithLogitsLoss()\n",
        "unet_optimizer = optim.AdamW(unet_model.parameters(), lr=LEARNING_RATE)\n",
        "# scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer=unet_optimizer, step_size=SCHEDULER_STEP, gamma=SCHEDULER_GAMMA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T09:21:20.978959Z",
          "iopub.execute_input": "2021-07-14T09:21:20.979477Z"
        },
        "trusted": true,
        "id": "1XCttcl1Yq9R"
      },
      "source": [
        "train_loss_values, val_loss_values, avg_score_values = train(unet_model, unet_optimizer, bce_loss, max_epochs, batch_size, data_tr, data_val, iou_pytorch, lr_sched=scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZrihNK8Yq9S"
      },
      "source": [
        "score_model(unet_model, iou_pytorch, data_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n84xwDEGYq9S"
      },
      "source": [
        "## Save loss, score and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T07:32:13.914927Z",
          "iopub.execute_input": "2021-07-13T07:32:13.915262Z",
          "iopub.status.idle": "2021-07-13T07:32:13.92536Z",
          "shell.execute_reply.started": "2021-07-13T07:32:13.915232Z",
          "shell.execute_reply": "2021-07-13T07:32:13.924587Z"
        },
        "trusted": true,
        "id": "T5UE1WZHYq9T"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# map individual array elements to floats and then the map to a list\n",
        "train_loss_values_2save = (list(map(float,train_loss_values)))\n",
        "val_loss_values_2save = (list(map(float,val_loss_values)))\n",
        "avg_score_values_2save = (list(map(float,avg_score_values)))\n",
        "\n",
        "# save loss and score as csv\n",
        "pd.DataFrame(train_loss_values_2save).to_csv('/kaggle/working/train_loss_values.csv', index = False)\n",
        "pd.DataFrame(val_loss_values_2save).to_csv('/kaggle/working/val_loss_values.csv', index = False)\n",
        "pd.DataFrame(avg_score_values_2save).to_csv('/kaggle/working/avg_score_values.csv', index = False)\n",
        "\n",
        "# save model\n",
        "torch.save(unet_model.state_dict(), '/kaggle/working/my-unet-model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic8zHmNOYq9U"
      },
      "source": [
        "states = {\n",
        "        'number of epochs': MAX_EPOCHS,\n",
        "        'state_dict': unet_model.state_dict(),\n",
        "        'optimizer': unet_optimizer.state_dict()\n",
        "         }\n",
        "torch.save(states, '/kaggle/working/my-unet-model-states.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYhGHxFnYq9V"
      },
      "source": [
        ""
      ]
    }
  ]
}